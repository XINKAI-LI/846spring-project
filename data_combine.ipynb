{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "noble-directory",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-cc4702575a8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfull_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"%20\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;31m#print(full_name) White API\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0methnic_req\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"http://www.name-prism.com/api_token/eth/json/4868133984c85190/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfull_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0methnic_req\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.8\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrace\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"API\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrace\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"White\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrace\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Hispanic\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/requests/models.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    896\u001b[0m                     \u001b[0;31m# used.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcomplexjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 357\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from datetime import datetime, timezone\n",
    "import requests\n",
    "csv_list = [\"redis.redis.csv\", 'microsoft.vscode.csv'] # [\"redis.redis.csv\"]\n",
    "\n",
    "\n",
    "##### feature index ##########\n",
    "pr_author_login_idx = 0  ### delete\n",
    "pr_author_name_idx = 1  ### replace with race\n",
    "pr_create_at_idx = 2  ### replace with notice time\n",
    "pr_merged_at_idx = 3  ### replace with review time\n",
    "pr_changed_files_idx = 4\n",
    "pr_LOC_idx = 5\n",
    "pr_reviewers_idx = 6   #### delete for hypo 1-4 , change for hypo 5\n",
    "pr_first_review_at_idx = 7  ### delete\n",
    "pr_review_commits_idx = 8\n",
    "pr_review_comments_idx = 9\n",
    "pr_commits_idx = 10\n",
    "\n",
    "repo_stars_idx = 11\n",
    "repo_created_at_idx = 12  ####  replace with repo time\n",
    "repo_external_contribution_idx = 13\n",
    "repo_team_size_idx = 14\n",
    "\n",
    "user_followers_idx = 15\n",
    "user_created_at_idx = 16   ### replace with user time\n",
    "user_experience_idx = 17\n",
    "user_repo_experience_idx = 18\n",
    "user_main_member_idx = 19\n",
    "user_watched_idx = 20\n",
    "user_succ_rate_idx = 21\n",
    "\n",
    "def set_string(reveiewers):\n",
    "    \n",
    "    reveiewers = reveiewers[1:-1]\n",
    "    reveiewer_list = reveiewers.split(\",\")\n",
    "    res = []\n",
    "    for name in reveiewer_list:\n",
    "        res.append(name[1:-1])\n",
    "    return res\n",
    "\n",
    "######### get the dictionary for user-name : race\n",
    "race_dic = {} # key name, value race\n",
    "\n",
    "for file_name in csv_list:\n",
    "    f_read = open(\"./csv_files/\" + file_name, 'r')\n",
    "    with f_read:\n",
    "        reader = csv.reader(f_read)\n",
    "        count = 0\n",
    "        for read_row in reader:\n",
    "            if count == 0:\n",
    "                count += 1\n",
    "                continue\n",
    "            \n",
    "            author_name = read_row[pr_author_name_idx]\n",
    "            race_dic[author_name] = \"Unkonwn\"\n",
    "            for reviewer_name in set_string(read_row[pr_reviewers_idx]):\n",
    "                race_dic[reviewer_name] = \"Unkonwn\"\n",
    "#print(race_dic.keys())\n",
    "\n",
    "for name in race_dic:\n",
    "    full_name = name.replace(\" \", \"%20\")\n",
    "    #print(full_name) White API\n",
    "    ethnic_req = requests.get(\"http://www.name-prism.com/api_token/eth/json/4868133984c85190/\"+full_name).json()\n",
    "    for race, value in ethnic_req.items():\n",
    "        if value >= 0.8 and (race == \"API\" or race == \"White\" or race == \"Hispanic\"):\n",
    "            race_dic[name] = race\n",
    "            break\n",
    "    time.sleep(0.1)\n",
    "\n",
    "print(len(race_dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convenient-irrigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### build the data for hypo 1-4 ##########\n",
    "\n",
    "### the new variables for model\n",
    "pr_head1 = ['pr_author_race','pr_notice_time','pr_review_time','pr_changed_files',\n",
    "    'pr_LOC','pr_review_commits','pr_review_comments','pr_commits'] # 8\n",
    "\n",
    "\n",
    "repo_head = ['repo_stars','repo_create_time','repo_external_contribution','repo_team_size'] # 4\n",
    "\n",
    "user_head = ['user_followers','user_dev_time','user_experience', 'user_repo_experience','user_main_member',\n",
    "             'user_watched','user_succ_rate'] # 7\n",
    "\n",
    "csv_head1 = pr_head1 + repo_head + user_head\n",
    "\n",
    "length = len(csv_head)\n",
    "cur_time = datetime.now(timezone.utc)\n",
    "\n",
    "\n",
    "with open(\"combine_data.csv\", 'w') as f_write:\n",
    "    csv_write = csv.writer(f_write)\n",
    "    csv_head = csv_head1\n",
    "    csv_write.writerow(csv_head)\n",
    "    \n",
    "    for file_name in csv_list:\n",
    "        f_read = open(\"./csv_files/\" + file_name, 'r')\n",
    "        with f_read:\n",
    "            reader = csv.reader(f_read)\n",
    "            count = 0\n",
    "            for read_row in reader:\n",
    "                if count == 0:\n",
    "                    count += 1\n",
    "                    continue\n",
    "                write_row = []\n",
    "\n",
    "                \n",
    "                ######## calculate the time difference ##########\n",
    "                created_at = datetime.strptime(read_row[pr_create_at_idx], \"%Y-%m-%dT%H:%M:%S%z\")\n",
    "                merged_at = datetime.strptime(read_row[pr_merged_at_idx], \"%Y-%m-%dT%H:%M:%S%z\") \n",
    "                review_at =  datetime.strptime(read_row[pr_first_review_at_idx] + \"Z\", \"%Y-%m-%d %H:%M:%S%z\") \n",
    "                repo_created_at = datetime.strptime(read_row[repo_created_at_idx], \"%Y-%m-%dT%H:%M:%S%z\") \n",
    "                user_created_at = datetime.strptime(read_row[user_created_at_idx], \"%Y-%m-%dT%H:%M:%S%z\") \n",
    "                \n",
    "                pr_notice_time = merged_at - review_at\n",
    "                pr_review_time = review_at - created_at\n",
    "                repo_create_time = cur_time - repo_created_at\n",
    "                user_create_time = cur_time - user_created_at\n",
    "                ######### get the author race ##########\n",
    "                author_name = read_row[pr_author_name_idx]\n",
    "                pr_author_race = race_dic[author_name]\n",
    "\n",
    "                ## pr features\n",
    "                write_row.append(pr_author_race)\n",
    "                write_row.append(pr_notice_time.days)\n",
    "                write_row.append(pr_review_time.days)\n",
    "                write_row.append(read_row[pr_changed_files_idx])\n",
    "                write_row.append(read_row[pr_LOC_idx])\n",
    "                write_row.append(read_row[pr_review_commits_idx])\n",
    "                write_row.append(read_row[pr_review_comments_idx])\n",
    "                write_row.append(read_row[pr_commits_idx])\n",
    "                \n",
    "                ## repo features\n",
    "                write_row.append(read_row[repo_stars_idx])\n",
    "                write_row.append(repo_create_time.days)\n",
    "                write_row.append(read_row[repo_external_contribution_idx])\n",
    "                write_row.append(read_row[repo_team_size_idx])\n",
    "\n",
    "                ## user features\n",
    "                write_row.append(read_row[user_followers_idx])\n",
    "                write_row.append(user_create_time.days)\n",
    "                write_row.append(read_row[user_experience_idx])\n",
    "                write_row.append(read_row[user_repo_experience_idx])\n",
    "                write_row.append(read_row[user_main_member_idx])\n",
    "                write_row.append(read_row[user_watched_idx])\n",
    "                write_row.append(read_row[user_succ_rate_idx])\n",
    "                \n",
    "                csv_write.writerow(write_row)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "curious-fortune",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### build the data for hypo 5 ##########\n",
    "pr_head5 = ['pr_author_race','pr_notice_time','pr_review_time','pr_changed_files',\n",
    "    'pr_LOC','pr_reviewer_race','pr_review_commits','pr_review_comments','pr_commits'] # 9\n",
    "\n",
    "csv_head5 = pr_head5 + repo_head + user_head\n",
    "\n",
    "with open(\"combine_data_h5.csv\", 'w') as f_write:\n",
    "    csv_write = csv.writer(f_write)\n",
    "    csv_head = csv_head5\n",
    "    csv_write.writerow(csv_head)\n",
    "    \n",
    "    for file_name in csv_list:\n",
    "        f_read = open(\"./csv_files/\" + file_name, 'r')\n",
    "        with f_read:\n",
    "            reader = csv.reader(f_read)\n",
    "            count = 0\n",
    "            for read_row in reader:\n",
    "                if count == 0:\n",
    "                    count += 1\n",
    "                    continue\n",
    "                write_row = []\n",
    "\n",
    "                \n",
    "                ######## calculate the time difference ##########\n",
    "                created_at = datetime.strptime(read_row[pr_create_at_idx], \"%Y-%m-%dT%H:%M:%S%z\")\n",
    "                merged_at = datetime.strptime(read_row[pr_merged_at_idx], \"%Y-%m-%dT%H:%M:%S%z\") \n",
    "                review_at =  datetime.strptime(read_row[pr_first_review_at_idx] + \"Z\", \"%Y-%m-%d %H:%M:%S%z\") \n",
    "                repo_created_at = datetime.strptime(read_row[repo_created_at_idx], \"%Y-%m-%dT%H:%M:%S%z\") \n",
    "                user_created_at = datetime.strptime(read_row[user_created_at_idx], \"%Y-%m-%dT%H:%M:%S%z\") \n",
    "                \n",
    "                pr_notice_time = merged_at - review_at\n",
    "                pr_review_time = review_at - created_at\n",
    "                repo_create_time = cur_time - repo_created_at\n",
    "                user_create_time = cur_time - user_created_at\n",
    "                ######### get the author and reviewer race ##########\n",
    "                author_name = read_row[pr_author_name_idx]\n",
    "                pr_author_race = race_dic[author_name]\n",
    "                \n",
    "                reviewer_list = set_string(read_row[pr_reviewers_idx])\n",
    "                if len(reviewer_list) != 1:\n",
    "                    continue\n",
    "                reviewer_race = race_dic[reviewer_list[0]]\n",
    "                \n",
    "                ## pr features\n",
    "                write_row.append(pr_author_race)\n",
    "                write_row.append(pr_notice_time.days)\n",
    "                write_row.append(pr_review_time.days)\n",
    "                write_row.append(read_row[pr_changed_files_idx])\n",
    "                write_row.append(read_row[pr_LOC_idx])\n",
    "                write_row.append(reviewer_race)\n",
    "                write_row.append(read_row[pr_review_commits_idx])\n",
    "                write_row.append(read_row[pr_review_comments_idx])\n",
    "                write_row.append(read_row[pr_commits_idx])\n",
    "                \n",
    "                ## repo features\n",
    "                write_row.append(read_row[repo_stars_idx])\n",
    "                write_row.append(repo_create_time.days)\n",
    "                write_row.append(read_row[repo_external_contribution_idx])\n",
    "                write_row.append(read_row[repo_team_size_idx])\n",
    "\n",
    "                ## user features\n",
    "                write_row.append(read_row[user_followers_idx])\n",
    "                write_row.append(user_create_time.days)\n",
    "                write_row.append(read_row[user_experience_idx])\n",
    "                write_row.append(read_row[user_repo_experience_idx])\n",
    "                write_row.append(read_row[user_main_member_idx])\n",
    "                write_row.append(read_row[user_watched_idx])\n",
    "                write_row.append(read_row[user_succ_rate_idx])\n",
    "                \n",
    "                csv_write.writerow(write_row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
